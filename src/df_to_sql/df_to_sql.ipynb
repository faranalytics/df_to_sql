{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import typing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import orm\n",
    "import alembic as al\n",
    "from alembic import migration\n",
    "from alembic import operations\n",
    "from logger import logger\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_regexes = [r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}([+-]\\d{2}:\\d{2}|Z)?$']\n",
    "number_regexes = [r'^([1-9][0-9]+|[0-9])(\\.[0-9]+$|$)']\n",
    "\n",
    "\n",
    "def is_datetime(ds:pd.Series) -> bool:\n",
    "\n",
    "    try:\n",
    "        assert ds.dropna().shape[0] != 0\n",
    "        assert all([ds.str.match(regex).all() for regex in date_regexes])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_numeric(ds:pd.Series) -> bool:\n",
    "\n",
    "    try:\n",
    "        assert ds.dropna().shape[0] != 0\n",
    "        assert all([ds.str.match(regex).all() for regex in number_regexes])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_string(ds:pd.Series) -> bool:\n",
    "\n",
    "    return ds.dropna().shape[0] != 0\n",
    "\n",
    "\n",
    "def get_sql_type(ds) -> sa.sql.visitors.Traversible:\n",
    "        \n",
    "    if is_numeric(ds):\n",
    "        \n",
    "        whole = ds.str.extract(r'^([0-9]+)', expand=False).dropna().str.count(r'[0-9]').max()\n",
    "\n",
    "        fractional = ds.str.extract(r'(?<=\\.)([0-9]+)', expand=False).dropna().str.count(r'[0-9]').max()\n",
    "\n",
    "        fractional = 0 if np.isnan(fractional) else fractional\n",
    "\n",
    "        return sa.types.Numeric(precision=whole + fractional, scale=fractional)\n",
    "\n",
    "    elif is_datetime(ds):\n",
    "\n",
    "        return sa.types.DateTime()\n",
    "\n",
    "    elif is_string(ds):\n",
    "\n",
    "        length = ds.dropna().str.len().max()\n",
    "\n",
    "        return sa.types.String(length=length)\n",
    "\n",
    "    else:\n",
    "        return sa.types.Numeric(precision=1, scale=0)\n",
    "\n",
    "\n",
    "def get_tables(conn:sa.engine.Connection) -> dict[str, sa.schema.Table]:\n",
    "    metadata_obj = sa.MetaData(bind=conn)\n",
    "    metadata_obj.reflect(bind=conn)\n",
    "    return metadata_obj.tables\n",
    "\n",
    "\n",
    "def get_table(table_name:str, conn:sa.engine.Connection) -> sa.schema.Table:\n",
    "    tables = get_tables(conn=conn)\n",
    "    return tables[table_name]\n",
    "\n",
    "\n",
    "def get_table_names(conn:sa.engine.Connection) -> list[str]:\n",
    "    tables = get_tables(conn=conn)\n",
    "    return list(tables.keys())\n",
    "\n",
    "\n",
    "def get_columns(table_name:str, conn:sa.engine.Connection) -> list[sa.schema.Column]:\n",
    "    metadata_obj = sa.MetaData(bind=conn)\n",
    "    metadata_obj.reflect(bind=conn, only=[table_name])\n",
    "    table:sa.schema.Table = metadata_obj.tables[table_name]\n",
    "    return table.columns\n",
    "\n",
    "\n",
    "def get_column(table_name:str, column_name:str, conn:sa.engine.Connection) -> sa.schema.Column:\n",
    "    columns = get_columns(table_name=table_name, conn=conn)\n",
    "    return columns[column_name]\n",
    "\n",
    "\n",
    "def get_column_names(table_name:str, conn:sa.engine.Connection) -> list[str]:\n",
    "    return [column.name for column in get_columns(table_name=table_name, conn=conn)]\n",
    "\n",
    "\n",
    "def modify_column(table_name:str, vendor_column:sa.schema.Column, migration_column:sa.schema.Column, op:operations.Operations) -> None:\n",
    "\n",
    "    try:\n",
    "        trans = op.migration_context.connection.begin()\n",
    "        \n",
    "        op.add_column(\n",
    "        table_name=table_name, \n",
    "        column=migration_column\n",
    "        )\n",
    "\n",
    "        vendor_table = get_table(table_name=table_name, conn=op.migration_context.connection)\n",
    "        \n",
    "        cast = sa.cast(vendor_column, migration_column.type)\n",
    "\n",
    "        op.migration_context.connection.execute(vendor_table.update().values(**{migration_column.name:cast}))\n",
    "        \n",
    "        op.drop_column(table_name=table_name, column_name=vendor_column.name)\n",
    "\n",
    "        op.alter_column(\n",
    "        table_name=table_name, \n",
    "        column_name=migration_column.name, \n",
    "        new_column_name=vendor_column.name\n",
    "        )\n",
    "\n",
    "        trans.commit()\n",
    "    except BaseException as e:\n",
    "        logger.error(f\"Rollback attempt to change the type of the column named {vendor_column.name} from {vendor_column.type} to {migration_column.type}.\")\n",
    "        trans.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        if migration_column.name in get_column_names(table_name=table_name, conn=op.migration_context.connection):\n",
    "            op.drop_column(table_name=table_name, column_name=migration_column.name)\n",
    "        \n",
    "\n",
    "def get_migration_type(ds:pd.Series, vendor_column:sa.schema.Column, op:operations.Operations) -> typing.Union[sa.sql.visitors.Traversible, None]:\n",
    "\n",
    "    if is_numeric(ds) and isinstance(vendor_column.type, sa.types.Numeric):\n",
    "\n",
    "        import_type_whole = ds.str.extract(r'^([0-9]+)', expand=False).dropna().str.count(r'[0-9]').max()\n",
    "        import_type_whole = 1 if np.isnan(import_type_whole) else import_type_whole\n",
    "        \n",
    "        import_type_fractional = ds.str.extract(r'(?<=\\.)([0-9]+)', expand=False).dropna().str.count(r'[0-9]').max()\n",
    "        import_type_fractional = 0 if np.isnan(import_type_fractional) else import_type_fractional\n",
    "        \n",
    "        vendor_type_whole = vendor_column.type.precision-vendor_column.type.scale\n",
    "\n",
    "        if import_type_whole > vendor_type_whole or import_type_fractional > vendor_column.type.scale:\n",
    "\n",
    "            max_whole = max(import_type_whole, vendor_type_whole)\n",
    "            max_fractional = max(import_type_fractional, vendor_column.type.scale)\n",
    "            return sa.types.Numeric(precision=max_whole + max_fractional, scale=max_fractional)\n",
    "\n",
    "    elif is_datetime(ds) and isinstance(vendor_column.type, sa.types.DateTime):\n",
    "        return\n",
    "    elif is_string(ds):\n",
    "        import_type_length = ds.dropna().str.len().max()\n",
    "\n",
    "        if isinstance(vendor_column.type, sa.types.String):\n",
    "            if import_type_length > vendor_column.type.length:\n",
    "                return sa.types.String(length=import_type_length)\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            select = sa.select(sa.func.max(sa.func.length(sa.type_coerce(vendor_column, sa.types.String()))))\n",
    "            vendor_type_length = op.migration_context.connection.execute(select).first()[0]\n",
    "            if vendor_type_length is None:\n",
    "                vendor_type_length = 0\n",
    "            return sa.types.String(length=max(import_type_length, vendor_type_length))\n",
    "    elif ds.isna().all():\n",
    "        return\n",
    "    else:\n",
    "        raise Exception(f\"Unhandled type for the column named {vendor_column} of the type {vendor_column.type}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sql(df:pd.DataFrame, table_name:str, schema:str, engine:sa.engine.Engine) -> None:\n",
    "\n",
    "     try:\n",
    "          logger.info(f\"Begin sql import into the table named {table_name}.\")\n",
    "\n",
    "          conn:sa.engine.Connection = engine.connect()\n",
    "          op = operations.Operations(migration.MigrationContext.configure(conn))\n",
    "          \n",
    "          if table_name.isupper():\n",
    "               table_name = table_name.lower()\n",
    "               #  SA table names are lowercase if the name is all one case; hence, convert the table_name to lowercase.\n",
    "\n",
    "          assert not df.columns.duplicated().any()\n",
    "          \n",
    "          df.columns = [column_name.lower() if column_name.isupper() else column_name for column_name in df.columns]\n",
    "\n",
    "          if table_name not in get_table_names(conn=conn):\n",
    "               logger.debug(f\"Create new table for the table named {table_name}.\")\n",
    "               op.create_table(\n",
    "                    table_name, \n",
    "                    *[sa.schema.Column(import_column_name, get_sql_type(df[import_column_name]), nullable=True) for import_column_name in df.columns]\n",
    "                    )\n",
    "          else:\n",
    "               #  Begin migration.\n",
    "               for import_column_name in df.columns:\n",
    "                    #  The import column is already in the vendor table; hence, just add the column - a type change isn't needed.\n",
    "                    if import_column_name not in get_column_names(table_name=table_name, conn=conn):\n",
    "                         logger.debug(f\"Create the column named {import_column_name} in the table named {table_name}\")\n",
    "                         op.add_column(\n",
    "                              table_name=table_name, \n",
    "                              column=sa.schema.Column(import_column_name, get_sql_type(df[import_column_name]), nullable=True)\n",
    "                              )\n",
    "                    else:\n",
    "                         #  The import column already exists in the table; hence, determine if a type change is needed.\n",
    "                         vendor_column = get_column(table_name=table_name, column_name=import_column_name, conn=conn)\n",
    "                         migration_type = get_migration_type(ds=df[import_column_name], vendor_column=vendor_column, op=op)\n",
    "                         if migration_type is not None:\n",
    "\n",
    "                              column_names = get_column_names(table_name=table_name, conn=op.migration_context.connection)\n",
    "                              migration_column_name = hashlib.md5(''.join(column_names).encode()).hexdigest()[0:16]\n",
    "                              \n",
    "                              if migration_column_name in column_names:\n",
    "                                   raise Exception(f\"The migration column name {migration_column_name} is in the vendor table named {table_name}.\")\n",
    "                              \n",
    "                              migration_column = sa.schema.Column(\n",
    "                              name=migration_column_name,\n",
    "                              type_=migration_type,\n",
    "                              nullable=True\n",
    "                              )\n",
    "\n",
    "                              logger.debug(f\"Change the type of the column named {vendor_column.name} in the table named {table_name} from {repr(vendor_column.type)} to {repr(migration_column.type)}.\")\n",
    "                              \n",
    "                              modify_column(table_name=table_name, vendor_column=vendor_column, migration_column=migration_column, op=op)\n",
    "\n",
    "          vendor_table = get_table(table_name=table_name, conn=conn)\n",
    "          \n",
    "          for vendor_column in vendor_table.columns:\n",
    "          #  The migration of the vendor table has completed; hence, type the dataframe according to the types in the vendor table.\n",
    "\n",
    "               if vendor_column.name in df.columns:\n",
    "                    \n",
    "                    if isinstance(vendor_column.type, sa.types.Numeric):\n",
    "\n",
    "                         df.loc[df[vendor_column.name].notna(), vendor_column.name] = df.loc[df[vendor_column.name].notna(), vendor_column.name].apply(decimal.Decimal)\n",
    "\n",
    "                    elif isinstance(vendor_column.type, sa.types.DateTime):\n",
    "\n",
    "                         df.loc[df[vendor_column.name].notna(), vendor_column.name] = pd.to_datetime(df.loc[df[vendor_column.name].notna(), vendor_column.name], utc=True)\n",
    "          \n",
    "          df = df.drop_duplicates()\n",
    "               \n",
    "          records = df.replace({np.nan:None}).to_dict('records')\n",
    "          #  SA interprets None as NULL.\n",
    "\n",
    "          if len(records) > 0:\n",
    "               try:\n",
    "                    trans = conn.begin()\n",
    "\n",
    "                    temp_table_name = hashlib.md5(table_name.encode()).hexdigest()[0:16]\n",
    "                    if temp_table_name in get_table_names(conn=conn):\n",
    "                         raise Exception(f\"The table named {temp_table_name} is already present.\")\n",
    "\n",
    "                    logger.debug(f\"Create the temporary table named {temp_table_name}.\")\n",
    "\n",
    "                    temp_table_columns = [sa.schema.Column(column.name, column.type, nullable=column.nullable) for column in vendor_table.columns]\n",
    "                    temp_table = op.create_table(temp_table_name, *temp_table_columns)\n",
    "                    #  The columns that comprise temp_table_columns will form an auto-generated index; hence it is not needed to create an Index.\n",
    "\n",
    "                    logger.debug(f\"Insert records into the temporary table named {temp_table_name}.\")\n",
    "\n",
    "                    op.bulk_insert(temp_table, records)\n",
    "\n",
    "                    logger.debug(f\"Select and insert unique records into the table named {table_name}.\")\n",
    "\n",
    "                    select:sa.sql.expression.CompoundSelect = sa.sql.expression.except_(sa.select(temp_table), sa.select(vendor_table))\n",
    "                    conn.execute(sa.insert(vendor_table).from_select([*vendor_table.columns], select))\n",
    "                    \n",
    "                    trans.commit()\n",
    "               except BaseException as e:\n",
    "                    logger.error(f\"Rollback attempt to insert new records into {table_name}.\")\n",
    "                    trans.rollback()\n",
    "                    raise e\n",
    "               finally:\n",
    "                    if temp_table_name in get_table_names(conn=conn):\n",
    "                         logger.debug(f\"Drop the temporary table named {temp_table_name}.\")\n",
    "                         op.drop_table(temp_table_name, schema=schema)\n",
    "     finally:\n",
    "          conn.close()\n",
    "          logger.info(f\"End sql import procedure into the table named {table_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('python-3.9': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbe85667dd15df860125a42315fe0131ecfe115361c05d68f48a49224bbf16c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
